# @package _global_

# Override defaults: take configs from relative path
defaults:
  - override /model: parco.yaml
  - override /env: ffsp/ffsp20.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  #- override /logger: null # comment this line to enable logging
  - override /logger: wandb.yaml


# Model hyperparameters
model:
  _target_: "rl4co.models.zoo.pomo.POMO"
  num_augment: 0
  num_starts: 24
  policy: # note: all other arguments (e.g. embeddings) are automatically taken from the env
    # otherwise, you may pass init_embedding / context_embedding (example below)
    _target_: "parco.models.policy.PARCOMultiStagePolicy"
    num_stages: "${env.generator_params.num_stage}"
    env_name: "${env.name}"
    agent_handler: "highprob"
    embed_dim: 256
    num_encoder_layers: 3
    num_heads: 16
    ms_hidden_dim: 32
    feedforward_hidden: 512
    bias: False
    normalization: "instance"
    use_pos_token: True
    scale_factor: 10
    init_embedding_kwargs:
      embed_dim: ${model.policy.embed_dim}
      one_hot_seed_cnt: "${env.generator_params.num_machine}"
    context_embedding_kwargs:
      use_comm_layer: True
      num_heads: ${model.policy.num_heads}
      normalization: ${model.policy.normalization}
      feedforward_hidden: ${model.policy.feedforward_hidden}
      bias: ${model.policy.bias}
    dynamic_embedding_kwargs:
      embed_dim: ${model.policy.embed_dim}
      scale_factor: ${model.policy.scale_factor}
    val_decode_type: "sampling"
    test_decode_type: "sampling"
    pointer_check_nan: False
    use_decoder_mha_mask: False
    use_ham_encoder: True
  batch_size: 50
  val_batch_size: 100
  test_batch_size: 100
  train_data_size: 1000
  val_data_size: 100
  test_data_size: 1000
  optimizer_kwargs:
    lr: 1e-4
    weight_decay: 0.0
  lr_scheduler:
    "CosineAnnealingLR"
  lr_scheduler_kwargs:
      T_max: ${trainer.max_epochs}
      eta_min: 0.0000001
  # lr_scheduler:
  #   "MultiStepLR"
  # lr_scheduler_kwargs:
  #   milestones: [80, 95]
  #   gamma: 0.1

  # NOTE: this should be done on the model side
  metrics:
    train: ["reward", "max_reward"]
    val: ["reward", "max_reward"]
    test: ["reward", "max_reward"]
    log_on_step: True


# Logging: we use Wandb in this case
logger:
  wandb:
    project: "parco-${env.name}"
    tags: ["parco", "${env.name}", "communication"]
    group: "${env.name}_n${env.generator_params.num_job}_m${env.generator_params.num_machine}"
    name: "parco"


seed: 1234

# Used to save the best model. However, we are not using this in the current setup
callbacks:
  model_checkpoint:
    monitor: "val/max_reward"