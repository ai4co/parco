{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARCO for the FFSP\n",
    "\n",
    "\n",
    "Learning a Parallel AutoRegressive policy for a Combinatorial Optimization problem: the Flexible Flow Shop Scheduling Problem (FFSP).\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ai4co/parco/blob/main/examples/3.quickstart-ffsp.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>  <a href=\"https://arxiv.org/abs/2409.0381\"><img src=\"https://img.shields.io/badge/arXiv-2409.03811-b31b1b.svg\" alt=\"Open In ArXiv\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "from rl4co.models import POMO\n",
    "from parco.envs import FFSPEnv\n",
    "from parco.models import PARCOMultiStagePolicy\n",
    "\n",
    "# Greedy rollouts over trained model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FFSPEnv(generator_params=dict(num_job=20, num_machine=4),\n",
    "              data_dir=\"\",\n",
    "              val_file=\"../data/omdcpdp/n50_m10_seed3333.npz\",\n",
    "              test_file=\"../data/omdcpdp/n50_m10_seed3333.npz\",\n",
    "              )            \n",
    "td_test_data = env.generator(batch_size=[3])\n",
    "td_init = env.reset(td_test_data.clone()).to(device)\n",
    "td_init_test = td_init.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here we declare our policy and our PARCO model (policy + environment + RL algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "\n",
    "# Policy is the neural network\n",
    "policy = PARCOMultiStagePolicy(num_stages=env.num_stage,\n",
    "                               env_name=env.name,\n",
    "                               embed_dim=emb_dim,\n",
    "                               num_heads=8,\n",
    "                               normalization=\"instance\",\n",
    "                               init_embedding_kwargs={\"one_hot_seed_cnt\": env.num_machine})\n",
    "\n",
    "# We refer to the model as the policy + the environment + training data (i.e. full RL algorithm)\n",
    "model = POMO(     \n",
    "    env, \n",
    "    policy=policy,\n",
    "    train_data_size=1000, \n",
    "    val_data_size=100,\n",
    "    test_data_size=1000,    \n",
    "    batch_size=50, \n",
    "    val_batch_size=100,\n",
    "    test_batch_size=100,        \n",
    "    num_starts=24,   \n",
    "    num_augment=0,      \n",
    "    optimizer_kwargs={'lr': 1e-4, 'weight_decay': 0},\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_pre = td_init_test.clone()\n",
    "\n",
    "policy = model.policy.to(device)\n",
    "out = policy(td_pre, env, decode_type=\"greedy\", return_actions=True)\n",
    "\n",
    "print(\"Average makespan: {:.2f}\".format(-out['reward'].mean().item()))\n",
    "for i in range(3):\n",
    "    print(f\"Schedule {i} makespan: {-out['reward'][i].item():.2f}\")\n",
    "    env.render(td_pre, idx=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In here we call the trainer and then fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RL4COTrainer(\n",
    "    max_epochs=5, # few epochs for demo\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1, # change this to your GPU number\n",
    "    logger=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the trained model\n",
    "\n",
    "Now, we take the testing instances from above and evaluate the trained model on them with different evaluation techniques:\n",
    "- Greedy: We take the action with the highest probability\n",
    "- Sampling: We sample from the probability distribution N times and take the best one\n",
    "- Augmentation: we first augment N times the state and then take the best action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy evaluation\n",
    "\n",
    "Here we simply take the solution with greedy decoding type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_post = td_init_test.clone()\n",
    "\n",
    "policy = model.policy.to(device)\n",
    "out = policy(td_post, env, decode_type=\"greedy\", return_actions=True)\n",
    "\n",
    "print(\"Average makespan: {:.2f}\".format(-out['reward'].mean().item()))\n",
    "for i in range(3):\n",
    "    print(f\"Schedule {i} makespan: {-out['reward'][i].item():.2f}\")\n",
    "    env.render(td_post, idx=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
